{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP03VEBDVMa1EoZjtIJVjpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oebyakarah/PDF-QA-Agent/blob/main/PDF_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Question-Answering Agent\n",
        "This is An AI tool that lets you upload a PDF and ask questions about it using Retrieval-Augmented Generation .\n"
      ],
      "metadata": {
        "id": "U-adelEal2YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community\n",
        "!pip install pypdf faiss-cpu\n",
        "!pip install transformers sentence-transformers\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uufyux4uGySj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ONoeVnV-HRsy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "id": "MdJdEdpGtJYS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.32.4 --force-reinstall"
      ],
      "metadata": {
        "collapsed": true,
        "id": "elUSpniYu4in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain_community pypdf faiss-cpu transformers sentence-transformers requests"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HiREZ4TLva6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_path = \"/content/AI_Whitepaper_Portfolio.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "print(f\"PDF loaded! Total pages: {len(documents)}\")"
      ],
      "metadata": {
        "id": "QihJPBrtko6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Number of chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z4KV9Fb1wuHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HCSpNi-lyYA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_community transformers faiss-cpu pypdf"
      ],
      "metadata": {
        "id": "g91a6ilhdXYZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "hf_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "query = \"Summarize the main points from chapter 1.\"\n",
        "result = qa_chain.run(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "BYgJcY11zwM_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "VTsoWteJ0tAn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Summarize the Methodolgy section.\"\n",
        "result = qa_chain.run(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4ObiQTlu0u3a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_output(text, width=100):\n",
        "    return \"\\n\".join(textwrap.wrap(str(text), width=width))\n",
        "\n",
        "questions = [\n",
        "    \"Summarize the Methodology section.\",\n",
        "    \"What are the key findings?\",\n",
        "    \"Explain the case study in simple terms.\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    raw_answer = qa_chain.run(question)\n",
        "    wrapped_answer = wrap_output(raw_answer)\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Answer:\\n\", wrapped_answer)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8sOcWeYBNZG",
        "outputId": "811d9c47-0e30-487c-b862-f6f39b7a2ad0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Summarize the Methodology section.\n",
            "Answer:\n",
            " Retrieval-Augmented Generation (RAG): A Practical Whitepaper Prepared by: [Your Name Here] Date:\n",
            "September 2025 This paper provides an overview of Agentic AI systems and their integration with\n",
            "Retrieval-Augmented Generation (RAG). It covers the core concepts, methodologies, and a step-by-step\n",
            "case study. This whitepaper is designed to demonstrate skills in Natural Language Processing (NLP),\n",
            "LangChain pipelines, and modern LLM techniques for real-world applications.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Question: What are the key findings?\n",
            "Answer:\n",
            " The system successfully answered domain-specific queries with a 92% accuracy rate in a controlled\n",
            "test environment. Compared to a baseline non-RAG LLM, our system showed a 40% improvement in factual\n",
            "correctness and significantly reduced hallucinations. Future Work Future improvements will focus on:\n",
            "- Scaling to multi-modal inputs (text, images, audio). - Implementing advanced reasoning through\n",
            "ReAct and tool-chaining. - Deploying to a agents capable of reasoning, planning, and executing\n",
            "tasks. However, LLMs are limited by their static training data. Retrieval-Augmented Generation (RAG)\n",
            "addresses this limitation by incorporating external knowledge dynamically during inference.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Question: Explain the case study in simple terms.\n",
            "Answer:\n",
            " Embedding generation using Hugging Face Agentic AI and Retrieval-Augmented Generation (RAG): A\n",
            "Practical Whitepaper Prepared by: [Your Name Here] Date: September 2025 This document provides an\n",
            "overview of Agentic AI systems and their integration with Retrieval-Augmented Generation (RAG). It\n",
            "covers the core concepts, methodologies, and a step-by-step case study. This whitepaper is designed\n",
            "to demonstrate skills in Natural Language Processing (NLP), LangChain pipelines, and modern LLM\n",
            "techniques for real-world applications. Abstract sentence-transformers. 3. Vector storage with FAISS\n",
            "for efficient retrieval. 4. LLM response generation via open models such as FLAN-T5. 5. Agentic\n",
            "orchestration with LangChain tools and memory.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}